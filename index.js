import mammoth from "mammoth";
import express from "express";
import multer from "multer";
import cors from "cors";
import { pipeline } from "@xenova/transformers";
import { createRequire } from "module";
import path from "path";

const require = createRequire(import.meta.url);
const pdfParse = require("pdf-parse"); // ‚úÖ Correct ESM import

const app = express();
const PORT = 5000;

app.use(cors());
app.use(express.json());

// ‚úÖ Use memory storage for multer
const storage = multer.memoryStorage();
const upload = multer({ storage });

// Load SBERT model
let embedder;
(async () => {
  console.log("‚è≥ Loading SBERT model...");
  embedder = await pipeline("feature-extraction", "Xenova/all-MiniLM-L6-v2");
  console.log("‚úÖ SBERT Model Loaded");

  app.listen(PORT, () => console.log(`üöÄ Backend running at http://localhost:${PORT}`));
})();

// Mean Pooling
function meanPool(tokenVectors) {
  const dimension = tokenVectors[0].length;
  const result = new Array(dimension).fill(0);
  for (const token of tokenVectors) {
    for (let i = 0; i < dimension; i++) {
      result[i] += token[i];
    }
  }
  return result.map(v => v / tokenVectors.length);
}

// Get embedding
async function getEmbedding(text) {
  const output = await embedder(text);
  const tokenVectors = output[0];
  return meanPool(tokenVectors);
}

// Extract text from PDF or DOCX/DOC
async function extractText(file) {
  try {
    const ext = path.extname(file.originalname).toLowerCase();
    const data = file.buffer; // ‚úÖ Use buffer directly

    if (ext === ".pdf") {
      console.log("Processing PDF:", file.originalname);
      const pdfData = await pdfParse(data);
      return pdfData.text;
    } else if (ext === ".docx" || ext === ".doc") {
      console.log("Processing DOCX/DOC:", file.originalname);
      const result = await mammoth.extractRawText({ buffer: data });
      return result.value;
    } else {
      throw new Error("Unsupported file type: " + file.originalname);
    }
  } catch (err) {
    console.error("Error extracting text from", file.originalname, err);
    return "";
  }
}

// Cosine similarity
function cosineSimilarity(vecA, vecB) {
  const dot = vecA.reduce((sum, v, i) => sum + v * vecB[i], 0);
  const magA = Math.sqrt(vecA.reduce((sum, v) => sum + v * v, 0));
  const magB = Math.sqrt(vecB.reduce((sum, v) => sum + v * v, 0));
  return dot / (magA * magB);
}

// Upload API
app.post("/upload", upload.array("resumes"), async (req, res) => {
  try {
    const jobDesc = req.body.jobDescription;
    const files = req.files;

    if (!jobDesc || !files?.length) {
      return res.status(400).json({ error: "Missing job description or resumes" });
    }

    const jobEmbedding = await getEmbedding(jobDesc);
    const results = [];

    for (const file of files) {
      const text = await extractText(file);
      if (!text) {
        console.log("Skipping file (empty text):", file.originalname);
        continue;
      }

      const resumeEmbedding = await getEmbedding(text);
      const score = cosineSimilarity(jobEmbedding, resumeEmbedding);

      results.push({
        name: file.originalname.replace(/\.(pdf|docx|doc)$/i, ""),
        score: (score * 100).toFixed(2),
      });
    }

    results.sort((a, b) => b.score - a.score);
    res.json({ ranked_resumes: results });
  } catch (err) {
    console.error("‚ùå Server Error:", err);
    res.status(500).json({ error: err.toString() });
  }
});

